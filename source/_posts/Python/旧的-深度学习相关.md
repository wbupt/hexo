---
title: 旧的-深度学习相关
tags: [Hexo, github]
published: true
hideInList: false
isTop: false
abbrlink: 20929
date: 2023-03-14 10:19:08
feature:
sticky: 
top: 
---
---
```python
import xlrd # Excel
import numpy as np
import matplotlib.pyplot as plt
from mlpNetWork import * #model
import os, sys
import json
import random
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm, trange #output


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
np.random.seed(0)
DEBUG = False

basedir = './logs'

expname = 'material_test'
lrate = 5e-4


def DataProcess():
    ####------------------- get the step length ---------------------#####
    workSheet = xlrd.open_workbook(r'wavelength.xlsx')
    wavelength = workSheet.sheet_by_name('wavelength')
    wavelength = wavelength.col_values(0)#get the step length
    #print(wavelength)
    #fig = plt.figure()
    #ax = fig.add_subplot(1, 1, 1)
    #for i in range(curve1.ncols):
    #    ax.plot(wavelength, curve1DataVector[i])
    #plt.show()
    #####--------------- get the curve information -----------------#####
    workSheet = xlrd.open_workbook(r'curve1.xlsx')
    curve1 = workSheet.sheet_by_name('curve1')
    #curve1 = curve1.col_values(0)
    curve1DataVector = [curve1.col_values(r) for r in range(curve1.ncols)]
    curve1DataVector = np.stack(curve1DataVector, 0) #get the curve information [3000 181]

    #fig = plt.figure()
    #ax = fig.add_subplot(1, 1, 1)
    #for i in range(curve1.ncols):
    #    ax.plot(wavelength, curve1DataVector[i])
    #plt.show()


    ####------------------- get the material -----------------------#####
    workSheet = xlrd.open_workbook(r'material.xlsx')
    material = workSheet.sheet_by_name('material')
    materialVector = [material.col_values(r) for r in range(material.ncols)]
    materialVector = np.stack(materialVector, -1) #get the material[3000 3]

    return wavelength, curve1DataVector, materialVector


def DataProcessMaterial():

    #####--------------- get the curve information -----------------#####
    workSheet = xlrd.open_workbook(r'input.xlsx')
    curve1 = workSheet.sheet_by_name('Sheet1')
    #curve1 = curve1.col_values(0)
    heightAndWide = [curve1.col_values(r) for r in range(curve1.ncols)]
    heightAndWide = np.stack(heightAndWide, 0) #get the curve information [3000 181]

    #fig = plt.figure()
    #ax = fig.add_subplot(1, 1, 1)
    #for i in range(curve1.ncols):
    #    ax.plot(wavelength, curve1DataVector[i])
    #plt.show()


    ####------------------- get the material -----------------------#####
    workSheet = xlrd.open_workbook(r'output.xlsx')
    material = workSheet.sheet_by_name('Sheet1')
    metricPerform = [material.col_values(r) for r in range(material.ncols)]
    metricPerform = np.stack(metricPerform, -1) #get the material[3000 3]

    return heightAndWide, metricPerform

def batchify(fn, chunk):
    """Constructs a version of 'fn' that applies to smaller batches.
    """
    if chunk is None:
        return fn
    def ret(inputs):
        return torch.cat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)
    return ret

def run_network(inputs, fn, embed_fn, netchunk=1024*64):
    inputs_flat = torch.reshape(inputs, [-1, inputs.shape[-1]])
    embedded = embed_fn(inputs_flat)

    outputs_flat = batchify(fn, netchunk)(embedded)
    outputs = torch.reshape(outputs_flat, list(inputs.shape[:-1]) + [outputs_flat.shape[-1]])
    return outputs

####-------------------- model the MLP NetWork -----------------#####
def create_mlp():

    multires = 10
    i_embed = 0

    embed_fn, input_ch = get_embedder(multires, i_embed)

    output_ch = 3
    skips = [4]
    netdepth = 8
    netwidth = 256

    modelMaterial = mlpNetwork(D=netdepth, W=netwidth,
                 input_ch=input_ch, output_ch=output_ch, skips=skips).to(device)

    print(modelMaterial)
    grad_vars = list(modelMaterial.parameters())

    netchunk = 1024 * 64
    network_query_fn = lambda inputs, network_fn : run_network(inputs, network_fn,
                                                                embed_fn=embed_fn,
                                                                netchunk=netchunk)
    # Create optimizer
    optimizer = torch.optim.Adam(params=grad_vars, lr=lrate, betas=(0.9, 0.999))

    start = 0
    global basedir
    global expname

    ##########################


    ##########################

    render_kwargs_train = {
        'network_query_fn' : network_query_fn,
        'network_fnMaterial' : modelMaterial,
    }
    render_kwargs_test = {k : render_kwargs_train[k] for k in render_kwargs_train}
    return render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer


####--------------------- MLP predict Data-------------------------####
def predictCurve(batchMaterial, network_query_fn, network_fnMaterial):
    rawRayStep = network_query_fn(batchMaterial, network_fnMaterial)
    predictStep = torch.sigmoid(rawRayStep)  # [N_rays, 64(predivt)]

    return predictStep

####-------------------------core process--------------------------####
def train():
    heightAndWide, metricPerform = DataProcessMaterial()
    heightAndWide, metricPerform = np.array(heightAndWide), np.array(metricPerform)
    heightAndWide = np.transpose(heightAndWide, [1,0])
    print('heightAndWide.shape = ', heightAndWide.shape, 'metricPerform.shape = ', metricPerform.shape)


    render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer = create_mlp()
    global_step = start



    #testData
    material_num_curve = np.concatenate([heightAndWide, metricPerform], -1) #[materialNum 3000, var 3 + curve 181]
    material_num_curve = material_num_curve.astype(np.float32)
    np.random.shuffle(material_num_curve)



    testMaterialData = material_num_curve[93]
    testMaterialData = torch.Tensor(testMaterialData).to(device)


    material_num_curve = np.delete(material_num_curve, 93, axis=0)
    material_num_curve = torch.Tensor(material_num_curve).to(device)


    N_iters = 15000 + 1

    start = start + 1
    i_print = 100

    global basedir
    global expname
    global lrate

    lrate_decay = 250
    N_rand = 100
    i_batch = 0
    i_test = 3000

    plt.ion()

    trainFlag = 0
    #数据测试
    if trainFlag == 0:
        print('cs')
        modelWeight = torch.load('networkFinal.ckpt')
        render_kwargs_train['network_fnMaterial'].load_state_dict(modelWeight)
        #read data
        workSheet = xlrd.open_workbook(r'input2.xlsx')
        testData = workSheet.sheet_by_name('Sheet1')
        testData = [testData.col_values(r) for r in range(testData.ncols)]
        testData = np.array(np.stack(testData, 0))  # get the curve information [3000 181]
        testData = np.transpose(testData, [1, 0])
        testData = testData.astype(np.float32)
        testData = torch.tensor(testData).to(device)

        with torch.no_grad():

            print(testData.shape)
            predictData = predictCurve(testData, **render_kwargs_train)

        predictData = predictData.cpu()
        np.savetxt('./1.txt', predictData)
        print(predictData)

    #数据训练
    if trainFlag:
        for i in trange(start, N_iters):

            batch = material_num_curve[i_batch: i_batch + N_rand]
            batchMaterial, batchTarget = batch[:, :5], batch[:, 5:]

            i_batch += N_rand
            if i_batch >= material_num_curve.shape[0]:
                rand_idx = torch.randperm(material_num_curve.shape[0])
                material_num_curve = material_num_curve[rand_idx]
                i_batch = 0

            predictData = predictCurve(batchMaterial, **render_kwargs_train)

            optimizer.zero_grad()
            img_loss = predict2mse(predictData, batchTarget)
            loss = img_loss
            loss.backward()
            optimizer.step()

            decay_rate = 0.1
            decay_steps = lrate_decay * 1000
            new_lrate = lrate * (decay_rate ** (global_step / decay_steps))
            for param_group in optimizer.param_groups:
                param_group['lr'] = new_lrate

            if i % i_print == 0:
                #tqdm.write("[TRAIN] Iter: {i} Loss: {loss.item()} ")
                print('[TRAIN] Iter: ', i, 'Loss: ', loss)

            if i % i_test == 0:
                with torch.no_grad():
                    test_material, test_curve = testMaterialData[:5], testMaterialData[5:]
                    test_material = test_material[None, :]
                    predictData = predictCurve(test_material, **render_kwargs_test)
                    #print('predictData = ', predictData)
                    #print('test_curve = ', test_curve)

                    test_curve_cpu = test_curve.cpu()

                    predictData_cpu = predictData.cpu()
                    predictData_cpu = predictData_cpu.detach().numpy()
                    #print(predictData_cpu.shape)

                    predictData_cpu = predictData_cpu[0, :]
                    print(test_curve_cpu, predictData_cpu)
                    # plt.clf()
                    # plt.plot(wavelength, test_curve_cpu)
                    # plt.plot(wavelength, predictData_cpu)
                    # plt.pause(0.5)
            if i == N_iters - 1:
                torch.save(render_kwargs_train['network_fnMaterial'].state_dict(), 'networkFinal.ckpt')
            global_step += 1

    plt.ioff()
    plt.show()

if __name__=='__main__':
    torch.set_default_tensor_type('torch.cuda.FloatTensor')
    train()
```

